{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edb7d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method.\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0cff337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor x:\n",
      "tensor([[[0.6506, 0.5452, 0.4177],\n",
      "         [0.7808, 0.9484, 0.7105],\n",
      "         [0.2714, 0.9145, 0.9046],\n",
      "         [0.0708, 0.0866, 0.5198],\n",
      "         [0.3952, 0.7225, 0.4597]],\n",
      "\n",
      "        [[0.1547, 0.9644, 0.5771],\n",
      "         [0.4492, 0.3180, 0.4338],\n",
      "         [0.2439, 0.5791, 0.9714],\n",
      "         [0.7821, 0.0414, 0.4141],\n",
      "         [0.3151, 0.0916, 0.4109]]])\n",
      "Shape of x: torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define parameters\n",
    "batch_size = 2\n",
    "context_length = 5  # Also serves as num_tokens for this example\n",
    "d_in = 3\n",
    "d_out = 4\n",
    "num_heads = 2\n",
    "assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "head_dim = d_out // num_heads  # 2\n",
    "dropout = 0.0\n",
    "qkv_bias = False\n",
    "\n",
    "# Create a batch of random inputs\n",
    "x = torch.rand(batch_size, context_length, d_in)\n",
    "print(\"Input tensor x:\")\n",
    "print(x)\n",
    "print(\"Shape of x:\", x.shape)  # Expected: (2, 4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b93c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries tensor:\n",
      "tensor([[[ 0.1977, -0.3273,  0.1582, -0.0144],\n",
      "         [ 0.3817, -0.4335,  0.1787,  0.1413],\n",
      "         [ 0.5048, -0.1818,  0.0891,  0.5025],\n",
      "         [ 0.1947,  0.0545,  0.1086,  0.2730],\n",
      "         [ 0.2869, -0.2679,  0.0666,  0.1609]],\n",
      "\n",
      "        [[ 0.4162, -0.2173, -0.0175,  0.3804],\n",
      "         [ 0.1734, -0.1793,  0.1494,  0.0653],\n",
      "         [ 0.4507, -0.0614,  0.1565,  0.5099],\n",
      "         [ 0.0586, -0.2365,  0.2842, -0.1491],\n",
      "         [ 0.1274, -0.0663,  0.1504,  0.0893]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Shape of queries: torch.Size([2, 5, 4])\n",
      "\n",
      "Keys tensor:\n",
      "tensor([[[-0.4958,  0.3919, -0.1996, -0.6609],\n",
      "         [-0.7172,  0.4835, -0.2045, -0.9477],\n",
      "         [-0.5063,  0.2617, -0.1046, -0.7034],\n",
      "         [-0.0921,  0.1712, -0.2130, -0.2117],\n",
      "         [-0.4594,  0.2353, -0.0436, -0.5871]],\n",
      "\n",
      "        [[-0.4576,  0.0979,  0.1017, -0.5638],\n",
      "         [-0.3291,  0.3204, -0.2247, -0.4772],\n",
      "         [-0.3737,  0.3171, -0.2647, -0.5959],\n",
      "         [-0.3617,  0.5415, -0.4623, -0.5681],\n",
      "         [-0.1881,  0.2749, -0.2589, -0.3209]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Shape of keys: torch.Size([2, 5, 4])\n",
      "\n",
      "Values tensor:\n",
      "tensor([[[ 0.1867, -0.3404, -0.0205,  0.3134],\n",
      "         [ 0.3778, -0.5711,  0.1260,  0.4295],\n",
      "         [ 0.4232, -0.6177,  0.3289,  0.3020],\n",
      "         [ 0.0082, -0.2327, -0.0157,  0.1265],\n",
      "         [ 0.3222, -0.3944,  0.1924,  0.2475]],\n",
      "\n",
      "        [[ 0.4876, -0.4963,  0.4297,  0.2038],\n",
      "         [ 0.0875, -0.2769, -0.0554,  0.2417],\n",
      "         [ 0.2342, -0.5524,  0.1512,  0.2910],\n",
      "         [-0.1169, -0.2082, -0.3602,  0.3322],\n",
      "         [-0.0190, -0.2010, -0.1182,  0.1841]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Shape of values: torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "W_query = nn.Linear(d_in, d_out, bias=qkv_bias) #create tensors for query key and value\n",
    "W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "queries = W_query(x)\n",
    "keys = W_key(x)\n",
    "values = W_value(x)\n",
    "\n",
    "print(\"Queries tensor:\")\n",
    "print(queries)\n",
    "print(\"Shape of queries:\", queries.shape)  # Expected: (2, 5, 4) - batch, tokens, d_out\n",
    "\n",
    "print(\"\\nKeys tensor:\")\n",
    "print(keys)\n",
    "print(\"Shape of keys:\", keys.shape)  # Expected: (2, 5, 4)\n",
    "\n",
    "print(\"\\nValues tensor:\")\n",
    "print(values)\n",
    "print(\"Shape of values:\", values.shape)  # Expected: (2, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f26116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped queries:\n",
      "tensor([[[[ 0.1977, -0.3273],\n",
      "          [ 0.1582, -0.0144]],\n",
      "\n",
      "         [[ 0.3817, -0.4335],\n",
      "          [ 0.1787,  0.1413]],\n",
      "\n",
      "         [[ 0.5048, -0.1818],\n",
      "          [ 0.0891,  0.5025]],\n",
      "\n",
      "         [[ 0.1947,  0.0545],\n",
      "          [ 0.1086,  0.2730]],\n",
      "\n",
      "         [[ 0.2869, -0.2679],\n",
      "          [ 0.0666,  0.1609]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4162, -0.2173],\n",
      "          [-0.0175,  0.3804]],\n",
      "\n",
      "         [[ 0.1734, -0.1793],\n",
      "          [ 0.1494,  0.0653]],\n",
      "\n",
      "         [[ 0.4507, -0.0614],\n",
      "          [ 0.1565,  0.5099]],\n",
      "\n",
      "         [[ 0.0586, -0.2365],\n",
      "          [ 0.2842, -0.1491]],\n",
      "\n",
      "         [[ 0.1274, -0.0663],\n",
      "          [ 0.1504,  0.0893]]]], grad_fn=<ViewBackward0>)\n",
      "Shape of reshaped queries: torch.Size([2, 5, 2, 2])\n",
      "\n",
      "Reshaped keys:\n",
      "tensor([[[[-0.4958,  0.3919],\n",
      "          [-0.1996, -0.6609]],\n",
      "\n",
      "         [[-0.7172,  0.4835],\n",
      "          [-0.2045, -0.9477]],\n",
      "\n",
      "         [[-0.5063,  0.2617],\n",
      "          [-0.1046, -0.7034]],\n",
      "\n",
      "         [[-0.0921,  0.1712],\n",
      "          [-0.2130, -0.2117]],\n",
      "\n",
      "         [[-0.4594,  0.2353],\n",
      "          [-0.0436, -0.5871]]],\n",
      "\n",
      "\n",
      "        [[[-0.4576,  0.0979],\n",
      "          [ 0.1017, -0.5638]],\n",
      "\n",
      "         [[-0.3291,  0.3204],\n",
      "          [-0.2247, -0.4772]],\n",
      "\n",
      "         [[-0.3737,  0.3171],\n",
      "          [-0.2647, -0.5959]],\n",
      "\n",
      "         [[-0.3617,  0.5415],\n",
      "          [-0.4623, -0.5681]],\n",
      "\n",
      "         [[-0.1881,  0.2749],\n",
      "          [-0.2589, -0.3209]]]], grad_fn=<ViewBackward0>)\n",
      "Shape of reshaped keys: torch.Size([2, 5, 2, 2])\n",
      "\n",
      "Reshaped values:\n",
      "tensor([[[[ 0.1867, -0.3404],\n",
      "          [-0.0205,  0.3134]],\n",
      "\n",
      "         [[ 0.3778, -0.5711],\n",
      "          [ 0.1260,  0.4295]],\n",
      "\n",
      "         [[ 0.4232, -0.6177],\n",
      "          [ 0.3289,  0.3020]],\n",
      "\n",
      "         [[ 0.0082, -0.2327],\n",
      "          [-0.0157,  0.1265]],\n",
      "\n",
      "         [[ 0.3222, -0.3944],\n",
      "          [ 0.1924,  0.2475]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4876, -0.4963],\n",
      "          [ 0.4297,  0.2038]],\n",
      "\n",
      "         [[ 0.0875, -0.2769],\n",
      "          [-0.0554,  0.2417]],\n",
      "\n",
      "         [[ 0.2342, -0.5524],\n",
      "          [ 0.1512,  0.2910]],\n",
      "\n",
      "         [[-0.1169, -0.2082],\n",
      "          [-0.3602,  0.3322]],\n",
      "\n",
      "         [[-0.0190, -0.2010],\n",
      "          [-0.1182,  0.1841]]]], grad_fn=<ViewBackward0>)\n",
      "Shape of reshaped values: torch.Size([2, 5, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Reshape queries, keys, and values to split into multiple heads\n",
    "# This adds a num_heads dimension and reduces the last dim to head_dim\n",
    "# Without reshaping we cant process attention independently per head.\n",
    "# The full d_out projection is split into num_heads smaller subspaces (each of size head_dim) allowing diverse attention patterns across heads.\n",
    "queries = queries.view(batch_size, context_length, num_heads, head_dim)\n",
    "keys = keys.view(batch_size, context_length, num_heads, head_dim)\n",
    "values = values.view(batch_size, context_length, num_heads, head_dim)\n",
    "\n",
    "print(\"Reshaped queries:\")\n",
    "print(queries)\n",
    "print(\"Shape of reshaped queries:\", queries.shape)  # Expected: (2, 5, 2, 2) - batch, tokens, heads, head_dim\n",
    "\n",
    "print(\"\\nReshaped keys:\")\n",
    "print(keys)\n",
    "print(\"Shape of reshaped keys:\", keys.shape)  # Expected: (2, 5, 2, 2)\n",
    "\n",
    "print(\"\\nReshaped values:\")\n",
    "print(values)\n",
    "print(\"Shape of reshaped values:\", values.shape)  # Expected: (2, 5, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554aa0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed queries:\n",
      "tensor([[[[ 0.1977, -0.3273],\n",
      "          [ 0.3817, -0.4335],\n",
      "          [ 0.5048, -0.1818],\n",
      "          [ 0.1947,  0.0545],\n",
      "          [ 0.2869, -0.2679]],\n",
      "\n",
      "         [[ 0.1582, -0.0144],\n",
      "          [ 0.1787,  0.1413],\n",
      "          [ 0.0891,  0.5025],\n",
      "          [ 0.1086,  0.2730],\n",
      "          [ 0.0666,  0.1609]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4162, -0.2173],\n",
      "          [ 0.1734, -0.1793],\n",
      "          [ 0.4507, -0.0614],\n",
      "          [ 0.0586, -0.2365],\n",
      "          [ 0.1274, -0.0663]],\n",
      "\n",
      "         [[-0.0175,  0.3804],\n",
      "          [ 0.1494,  0.0653],\n",
      "          [ 0.1565,  0.5099],\n",
      "          [ 0.2842, -0.1491],\n",
      "          [ 0.1504,  0.0893]]]], grad_fn=<TransposeBackward0>)\n",
      "Shape of transposed queries: torch.Size([2, 2, 5, 2])\n",
      "\n",
      "Transposed keys:\n",
      "tensor([[[[-0.4958,  0.3919],\n",
      "          [-0.7172,  0.4835],\n",
      "          [-0.5063,  0.2617],\n",
      "          [-0.0921,  0.1712],\n",
      "          [-0.4594,  0.2353]],\n",
      "\n",
      "         [[-0.1996, -0.6609],\n",
      "          [-0.2045, -0.9477],\n",
      "          [-0.1046, -0.7034],\n",
      "          [-0.2130, -0.2117],\n",
      "          [-0.0436, -0.5871]]],\n",
      "\n",
      "\n",
      "        [[[-0.4576,  0.0979],\n",
      "          [-0.3291,  0.3204],\n",
      "          [-0.3737,  0.3171],\n",
      "          [-0.3617,  0.5415],\n",
      "          [-0.1881,  0.2749]],\n",
      "\n",
      "         [[ 0.1017, -0.5638],\n",
      "          [-0.2247, -0.4772],\n",
      "          [-0.2647, -0.5959],\n",
      "          [-0.4623, -0.5681],\n",
      "          [-0.2589, -0.3209]]]], grad_fn=<TransposeBackward0>)\n",
      "Shape of transposed keys: torch.Size([2, 2, 5, 2])\n",
      "\n",
      "Transposed values:\n",
      "tensor([[[[ 0.1867, -0.3404],\n",
      "          [ 0.3778, -0.5711],\n",
      "          [ 0.4232, -0.6177],\n",
      "          [ 0.0082, -0.2327],\n",
      "          [ 0.3222, -0.3944]],\n",
      "\n",
      "         [[-0.0205,  0.3134],\n",
      "          [ 0.1260,  0.4295],\n",
      "          [ 0.3289,  0.3020],\n",
      "          [-0.0157,  0.1265],\n",
      "          [ 0.1924,  0.2475]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4876, -0.4963],\n",
      "          [ 0.0875, -0.2769],\n",
      "          [ 0.2342, -0.5524],\n",
      "          [-0.1169, -0.2082],\n",
      "          [-0.0190, -0.2010]],\n",
      "\n",
      "         [[ 0.4297,  0.2038],\n",
      "          [-0.0554,  0.2417],\n",
      "          [ 0.1512,  0.2910],\n",
      "          [-0.3602,  0.3322],\n",
      "          [-0.1182,  0.1841]]]], grad_fn=<TransposeBackward0>)\n",
      "Shape of transposed values: torch.Size([2, 2, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# Transpose to move the heads dimension before the tokens dimension\n",
    "# Matrix multiplication for @ operator expects the sequence length (tokens) to be the dimension over which we compute dots.\n",
    "# After transpose, shape becomes (batch, heads, tokens, head_dim)\n",
    "queries = queries.transpose(1, 2)\n",
    "keys = keys.transpose(1, 2)\n",
    "values = values.transpose(1, 2)\n",
    "\n",
    "print(\"Transposed queries:\")\n",
    "print(queries)\n",
    "print(\"Shape of transposed queries:\", queries.shape)  # Expected: (2, 2, 4, 2) - batch, heads, tokens, head_dim\n",
    "\n",
    "print(\"\\nTransposed keys:\")\n",
    "print(keys)\n",
    "print(\"Shape of transposed keys:\", keys.shape)  # Expected: (2, 2, 4, 2)\n",
    "\n",
    "print(\"\\nTransposed values:\")\n",
    "print(values)\n",
    "print(\"Shape of transposed values:\", values.shape)  # Expected: (2, 2, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259f7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys transposed for matmul:\n",
      "tensor([[[[-0.4958, -0.7172, -0.5063, -0.0921, -0.4594],\n",
      "          [ 0.3919,  0.4835,  0.2617,  0.1712,  0.2353]],\n",
      "\n",
      "         [[-0.1996, -0.2045, -0.1046, -0.2130, -0.0436],\n",
      "          [-0.6609, -0.9477, -0.7034, -0.2117, -0.5871]]],\n",
      "\n",
      "\n",
      "        [[[-0.4576, -0.3291, -0.3737, -0.3617, -0.1881],\n",
      "          [ 0.0979,  0.3204,  0.3171,  0.5415,  0.2749]],\n",
      "\n",
      "         [[ 0.1017, -0.2247, -0.2647, -0.4623, -0.2589],\n",
      "          [-0.5638, -0.4772, -0.5959, -0.5681, -0.3209]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "Shape of transposed keys: torch.Size([2, 2, 2, 5])\n",
      "\n",
      "Attention scores tensor:\n",
      "tensor([[[[-0.2263, -0.3000, -0.1858, -0.0742, -0.1678],\n",
      "          [-0.3591, -0.4833, -0.3067, -0.1094, -0.2773],\n",
      "          [-0.3215, -0.4499, -0.3032, -0.0776, -0.2747],\n",
      "          [-0.0752, -0.1133, -0.0843, -0.0086, -0.0766],\n",
      "          [-0.2472, -0.3353, -0.2154, -0.0723, -0.1948]],\n",
      "\n",
      "         [[-0.0221, -0.0187, -0.0064, -0.0306,  0.0015],\n",
      "          [-0.1291, -0.1705, -0.1181, -0.0680, -0.0908],\n",
      "          [-0.3499, -0.4944, -0.3628, -0.1254, -0.2989],\n",
      "          [-0.2021, -0.2809, -0.2034, -0.0809, -0.1650],\n",
      "          [-0.1196, -0.1661, -0.1202, -0.0483, -0.0974]]],\n",
      "\n",
      "\n",
      "        [[[-0.2117, -0.2066, -0.2245, -0.2682, -0.1380],\n",
      "          [-0.0969, -0.1145, -0.1217, -0.1598, -0.0819],\n",
      "          [-0.2123, -0.1680, -0.1879, -0.1963, -0.1016],\n",
      "          [-0.0500, -0.0950, -0.0969, -0.1492, -0.0760],\n",
      "          [-0.0648, -0.0632, -0.0686, -0.0820, -0.0422]],\n",
      "\n",
      "         [[-0.2162, -0.1776, -0.2220, -0.2080, -0.1175],\n",
      "          [-0.0216, -0.0648, -0.0785, -0.1062, -0.0597],\n",
      "          [-0.2715, -0.2785, -0.3453, -0.3620, -0.2041],\n",
      "          [ 0.1130,  0.0073,  0.0137, -0.0467, -0.0257],\n",
      "          [-0.0351, -0.0764, -0.0930, -0.1203, -0.0676]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Shape of attention scores: torch.Size([2, 2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Compute the attention scores (raw dot products before scaling/masking/softmax)\n",
    "# This is queries @ keys^T for each head.\n",
    "# tokens_q == tokens_k == context_length, so we get a square matrix per head per batch.\n",
    "# The reshaping/transposing earlier ensures this matrix mult is head-independent and efficient.\n",
    "attn_scores = queries @ keys.transpose(2, 3)  # Transpose keys to (batch, heads, head_dim, tokens)\n",
    "\n",
    "print(\"Keys transposed for matmul:\")\n",
    "print(keys.transpose(2, 3))\n",
    "print(\"Shape of transposed keys:\", keys.transpose(2, 3).shape)  # Expected: (2, 2, 2, 4)\n",
    "\n",
    "print(\"\\nAttention scores tensor:\")\n",
    "print(attn_scores)\n",
    "print(\"Shape of attention scores:\", attn_scores.shape)  # Expected: (2, 2, 4, 4) - batch, heads, tokens, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b965729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal mask tensor:\n",
      "tensor([[0., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "Shape of mask: torch.Size([5, 5])\n",
      "\n",
      "Boolean mask:\n",
      "tensor([[False,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False,  True,  True],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False, False, False, False]])\n",
      "Shape of boolean mask: torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Create the causal mask as in the __init__ method\n",
    "# self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "# This creates a causal mask which is a  triangular matrix of 1's (the u in triu means above the diagonal) and 0's elsewhere.\n",
    "\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(\"Causal mask tensor:\")\n",
    "print(mask)\n",
    "print(\"Shape of mask:\", mask.shape)  \n",
    "\n",
    "# Convert to boolean for masking (True where we want to mask (fill with -inf)\n",
    "mask_bool = mask.bool()[:context_length, :context_length]\n",
    "print(\"\\nBoolean mask:\")\n",
    "print(mask_bool)\n",
    "print(\"Shape of boolean mask:\", mask_bool.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a4a3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention scores:\n",
      "tensor([[[[-0.2263,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3591, -0.4833,    -inf,    -inf,    -inf],\n",
      "          [-0.3215, -0.4499, -0.3032,    -inf,    -inf],\n",
      "          [-0.0752, -0.1133, -0.0843, -0.0086,    -inf],\n",
      "          [-0.2472, -0.3353, -0.2154, -0.0723, -0.1948]],\n",
      "\n",
      "         [[-0.0221,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1291, -0.1705,    -inf,    -inf,    -inf],\n",
      "          [-0.3499, -0.4944, -0.3628,    -inf,    -inf],\n",
      "          [-0.2021, -0.2809, -0.2034, -0.0809,    -inf],\n",
      "          [-0.1196, -0.1661, -0.1202, -0.0483, -0.0974]]],\n",
      "\n",
      "\n",
      "        [[[-0.2117,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0969, -0.1145,    -inf,    -inf,    -inf],\n",
      "          [-0.2123, -0.1680, -0.1879,    -inf,    -inf],\n",
      "          [-0.0500, -0.0950, -0.0969, -0.1492,    -inf],\n",
      "          [-0.0648, -0.0632, -0.0686, -0.0820, -0.0422]],\n",
      "\n",
      "         [[-0.2162,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0216, -0.0648,    -inf,    -inf,    -inf],\n",
      "          [-0.2715, -0.2785, -0.3453,    -inf,    -inf],\n",
      "          [ 0.1130,  0.0073,  0.0137, -0.0467,    -inf],\n",
      "          [-0.0351, -0.0764, -0.0930, -0.1203, -0.0676]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Shape of masked attention scores: torch.Size([2, 2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Apply the mask to the attention scores\n",
    "# masking prevents attending to future tokens.\n",
    "# masked_fill_() sets positions where mask_bool is True to -inf, which will become 0 after softmax.\n",
    "attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "print(\"Masked attention scores:\")\n",
    "print(attn_scores)\n",
    "print(\"Shape of masked attention scores:\", attn_scores.shape)  # Still (2, 2, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d781e520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights after softmax:\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5219, 0.4781, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3417, 0.3121, 0.3462, 0.0000, 0.0000],\n",
      "          [0.2491, 0.2424, 0.2475, 0.2611, 0.0000],\n",
      "          [0.1949, 0.1831, 0.1993, 0.2205, 0.2022]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5073, 0.4927, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3456, 0.3120, 0.3424, 0.0000, 0.0000],\n",
      "          [0.2479, 0.2344, 0.2476, 0.2700, 0.0000],\n",
      "          [0.1986, 0.1922, 0.1985, 0.2089, 0.2018]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5031, 0.4969, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3280, 0.3384, 0.3337, 0.0000, 0.0000],\n",
      "          [0.2585, 0.2504, 0.2501, 0.2410, 0.0000],\n",
      "          [0.1999, 0.2001, 0.1994, 0.1975, 0.2031]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5076, 0.4924, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3396, 0.3380, 0.3224, 0.0000, 0.0000],\n",
      "          [0.2664, 0.2472, 0.2484, 0.2380, 0.0000],\n",
      "          [0.2062, 0.2003, 0.1979, 0.1941, 0.2015]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Shape of attention weights: torch.Size([2, 2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Scale the attention scores by sqrt(head_dim) to stabilize gradients (scaled dot-product attention) (i had to look this up)\n",
    "# Then apply softmax along the last dimension (across keys for each query)\n",
    "# Dot products can grow large with high dimensions, leading to small gradients after softmax which turns scores into probabilities between 0 and 1 (attention weights).\n",
    "scale_factor = keys.shape[-1] ** 0.5  # head_dim ** 0.5\n",
    "attn_weights = torch.softmax(attn_scores / scale_factor, dim=-1)\n",
    "\n",
    "print(\"Attention weights after softmax:\")\n",
    "print(attn_weights)\n",
    "print(\"Shape of attention weights:\", attn_weights.shape)  # Expected: (2, 2, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f1415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights after dropout:\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5219, 0.4781, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3417, 0.3121, 0.3462, 0.0000, 0.0000],\n",
      "          [0.2491, 0.2424, 0.2475, 0.2611, 0.0000],\n",
      "          [0.1949, 0.1831, 0.1993, 0.2205, 0.2022]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5073, 0.4927, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3456, 0.3120, 0.3424, 0.0000, 0.0000],\n",
      "          [0.2479, 0.2344, 0.2476, 0.2700, 0.0000],\n",
      "          [0.1986, 0.1922, 0.1985, 0.2089, 0.2018]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5031, 0.4969, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3280, 0.3384, 0.3337, 0.0000, 0.0000],\n",
      "          [0.2585, 0.2504, 0.2501, 0.2410, 0.0000],\n",
      "          [0.1999, 0.2001, 0.1994, 0.1975, 0.2031]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5076, 0.4924, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3396, 0.3380, 0.3224, 0.0000, 0.0000],\n",
      "          [0.2664, 0.2472, 0.2484, 0.2380, 0.0000],\n",
      "          [0.2062, 0.2003, 0.1979, 0.1941, 0.2015]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Shape after dropout: torch.Size([2, 2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Apply dropout to attention weights (though dropout=0.0, so no change here)\n",
    "# dropout basically drops a few random weights from the tensor so that during training we dont encounter overfitting.\n",
    "# In the class, self.dropout = nn.Dropout(dropout)\n",
    "dropout_layer = nn.Dropout(dropout)\n",
    "attn_weights = dropout_layer(attn_weights)\n",
    "\n",
    "print(\"Attention weights after dropout:\")\n",
    "print(attn_weights)\n",
    "print(\"Shape after dropout:\", attn_weights.shape)  # Still (2, 2, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46147e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors before transpose:\n",
      "tensor([[[[ 0.1867, -0.3404],\n",
      "          [ 0.2780, -0.4507],\n",
      "          [ 0.3282, -0.5084],\n",
      "          [ 0.2450, -0.4368],\n",
      "          [ 0.2569, -0.4251]],\n",
      "\n",
      "         [[-0.0205,  0.3134],\n",
      "          [ 0.0517,  0.3706],\n",
      "          [ 0.1448,  0.3457],\n",
      "          [ 0.1017,  0.2873],\n",
      "          [ 0.1210,  0.2811]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4876, -0.4963],\n",
      "          [ 0.2888, -0.3873],\n",
      "          [ 0.2677, -0.4408],\n",
      "          [ 0.1784, -0.3860],\n",
      "          [ 0.1347, -0.3467]],\n",
      "\n",
      "         [[ 0.4297,  0.2038],\n",
      "          [ 0.1909,  0.2225],\n",
      "          [ 0.1760,  0.2447],\n",
      "          [ 0.0526,  0.2654],\n",
      "          [ 0.0137,  0.2496]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Shape of context_vec: torch.Size([2, 2, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# Compute the context vectors: weighted sum of values using attention weights\n",
    "# attn_weights (b, heads, tokens_q, tokens_k) @ values (b, heads, tokens_k, head_dim) -> (b, heads, tokens_q, head_dim)\n",
    "# this aggregates information from relevant tokens per query.\n",
    "context_vec = attn_weights @ values\n",
    "\n",
    "print(\"Context vectors before transpose:\")\n",
    "print(context_vec)\n",
    "print(\"Shape of context_vec:\", context_vec.shape)  # Expected: (2, 2, 4, 2) - batch, heads, tokens, head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04432e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed context_vec:\n",
      "tensor([[[[ 0.1867, -0.3404],\n",
      "          [-0.0205,  0.3134]],\n",
      "\n",
      "         [[ 0.2780, -0.4507],\n",
      "          [ 0.0517,  0.3706]],\n",
      "\n",
      "         [[ 0.3282, -0.5084],\n",
      "          [ 0.1448,  0.3457]],\n",
      "\n",
      "         [[ 0.2450, -0.4368],\n",
      "          [ 0.1017,  0.2873]],\n",
      "\n",
      "         [[ 0.2569, -0.4251],\n",
      "          [ 0.1210,  0.2811]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4876, -0.4963],\n",
      "          [ 0.4297,  0.2038]],\n",
      "\n",
      "         [[ 0.2888, -0.3873],\n",
      "          [ 0.1909,  0.2225]],\n",
      "\n",
      "         [[ 0.2677, -0.4408],\n",
      "          [ 0.1760,  0.2447]],\n",
      "\n",
      "         [[ 0.1784, -0.3860],\n",
      "          [ 0.0526,  0.2654]],\n",
      "\n",
      "         [[ 0.1347, -0.3467],\n",
      "          [ 0.0137,  0.2496]]]], grad_fn=<TransposeBackward0>)\n",
      "Shape after transpose: torch.Size([2, 5, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Transpose back --> move heads after tokens to prepare for combining heads\n",
    "# (b, heads, tokens, head_dim) -> (b, tokens, heads, head_dim)\n",
    "context_vec = context_vec.transpose(1, 2)\n",
    "\n",
    "print(\"Transposed context_vec:\")\n",
    "print(context_vec)\n",
    "print(\"Shape after transpose:\", context_vec.shape)  # Expected: (2, 5, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c013faee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined context_vec:\n",
      "tensor([[[ 0.1867, -0.3404, -0.0205,  0.3134],\n",
      "         [ 0.2780, -0.4507,  0.0517,  0.3706],\n",
      "         [ 0.3282, -0.5084,  0.1448,  0.3457],\n",
      "         [ 0.2450, -0.4368,  0.1017,  0.2873],\n",
      "         [ 0.2569, -0.4251,  0.1210,  0.2811]],\n",
      "\n",
      "        [[ 0.4876, -0.4963,  0.4297,  0.2038],\n",
      "         [ 0.2888, -0.3873,  0.1909,  0.2225],\n",
      "         [ 0.2677, -0.4408,  0.1760,  0.2447],\n",
      "         [ 0.1784, -0.3860,  0.0526,  0.2654],\n",
      "         [ 0.1347, -0.3467,  0.0137,  0.2496]]], grad_fn=<ViewBackward0>)\n",
      "Shape after view: torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "# Combine the heads, reshape back to (b, tokens, d_out) by merging heads and head_dim\n",
    "#  We split into heads for multi-head diversity; now concatenate the head outputs back into full d_out.\n",
    "context_vec = context_vec.contiguous().view(batch_size, context_length, d_out)\n",
    "\n",
    "print(\"Combined context_vec:\")\n",
    "print(context_vec)\n",
    "print(\"Shape after view:\", context_vec.shape)  # Expected: (2, 5, 4) - batch, tokens, d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87eb48b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final context_vec after output projection:\n",
      "tensor([[[ 0.0787,  0.6367, -0.2230,  0.4223],\n",
      "         [ 0.1166,  0.6805, -0.1950,  0.5090],\n",
      "         [ 0.1025,  0.6753, -0.1976,  0.5708],\n",
      "         [ 0.0571,  0.6449, -0.2240,  0.5105],\n",
      "         [ 0.0575,  0.6312, -0.2254,  0.5138]],\n",
      "\n",
      "        [[ 0.0580,  0.5379, -0.2327,  0.6814],\n",
      "         [ 0.0375,  0.5728, -0.2434,  0.5246],\n",
      "         [ 0.0336,  0.6157, -0.2356,  0.5385],\n",
      "         [ 0.0337,  0.6351, -0.2368,  0.4610],\n",
      "         [ 0.0198,  0.6258, -0.2460,  0.4240]]], grad_fn=<ViewBackward0>)\n",
      "Shape of final context_vec: torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "# Finally, apply the output projection: a linear layer to mix the combined head outputs\n",
    "# In the class, self.out_proj = nn.Linear(d_out, d_out)\n",
    "# This is optional but allows learning a better combination of heads.\n",
    "out_proj = nn.Linear(d_out, d_out)  # Note: weights are random here, as in init\n",
    "\n",
    "context_vec = out_proj(context_vec)\n",
    "\n",
    "print(\"Final context_vec after output projection:\")\n",
    "print(context_vec)\n",
    "print(\"Shape of final context_vec:\", context_vec.shape)  # Expected: (2, 4, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
